{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the directories to read the data\n",
    "train_directory = 'root/train'\n",
    "test_directory = 'root/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter batch size. 4\n",
      "Classes:  ['1', '10', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "def loadTrainTest(train_dir, test_dir, batch_size):\n",
    "    # Transformation for image transforms.Grayscale(1), \n",
    "    transformation = transforms.Compose([ transforms.Grayscale(1),\n",
    "                                             transforms.Resize((28,28)),\n",
    "                                                 transforms.ToTensor(), \n",
    "                                                        transforms.Normalize((0.5, ), (0.5, ))])\n",
    "   # transformation1 = transforms.Compose([transforms.Resize(28),\n",
    "                                          #transforms.Grayscale(), transforms.ToTensor(), \n",
    "                                     #transforms.Normalize((0.5, ), (0.5, ))])\n",
    "    \n",
    "    # Load train and test dataset with ImageFolder\n",
    "    train_dataset = datasets.ImageFolder(root = train_dir, \n",
    "                                         transform = transformation)\n",
    "    test_dataset = datasets.ImageFolder(root = test_dir, \n",
    "                                        transform = transformation)\n",
    "    \n",
    "    # Load train and test dataset into batches\n",
    "    trainloader = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                         batch_size = batch_size,\n",
    "                                         shuffle = True)\n",
    "    testloader = torch.utils.data.DataLoader(dataset = test_dataset, \n",
    "                                         batch_size = batch_size,\n",
    "                                         shuffle = True)\n",
    "    return trainloader, testloader, train_dataset.classes\n",
    "\n",
    "\n",
    "batch_size = int(input('Enter batch size. '))\n",
    "train_load, test_load, classes = loadTrainTest(train_directory, test_directory, batch_size)\n",
    "\n",
    "print('Classes: ', classes) # print number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(image):\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image = image.numpy().transpose((1, 2, 0))  \n",
    "    else:\n",
    "        image = np.array(image).transpose((1, 2, 0))\n",
    "    # unnormalize\n",
    "    image = 0.5*image + 0.5\n",
    "    # Plot\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    \n",
    "# get some random training images\n",
    "images, labels = next(iter(train_load))\n",
    "imshow(torchvision.utils.make_grid(images))  # show images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (fc1): Linear(in_features=784, out_features=84, bias=True)\n",
      "  (fc2): Linear(in_features=84, out_features=50, bias=True)\n",
      "  (fc3): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(1*28*28, 84)\n",
    "        self.fc2 = nn.Linear(84, 50)\n",
    "        self.fc3 = nn.Linear(50,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1*28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "model = SimpleNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Write the type of loss function you want. \n",
      "                        'entropy' for CrossEntropyLoss \n",
      "                        'nll' for NLLLoss \n",
      "entropy\n",
      "\n",
      "Write the type of optimizer function you want.\n",
      "                        'sgd' for SGD \n",
      "                        'adam' for Adam\n",
      "                        'rms' for RMSprop\n",
      "                        'adagrad' for Adagrad \n",
      "sgd\n",
      "\n",
      "Selected loss function: entropy \n",
      "Selected optimizer function: sgd\n"
     ]
    }
   ],
   "source": [
    "def lossAndoptimizer(ls, op, lr = 1e-2):\n",
    "    loss_func = {'entropy' : nn.CrossEntropyLoss(), \n",
    "                         'nll': nn.NLLLoss() }\n",
    "    \n",
    "    optimizer_func = {'sgd': optim.SGD(model.parameters(), lr), \n",
    "                          'adam': optim.Adam(model.parameters(), lr),\n",
    "                             'adagrad': optim.Adagrad(model.parameters(), lr),\n",
    "                                 'rms': optim.RMSprop(model.parameters(), lr) }\n",
    "    \n",
    "    return loss_func[ls], optimizer_func[op]\n",
    "\n",
    "\n",
    "loss_f = input(''' Write the type of loss function you want. \n",
    "                        'entropy' for CrossEntropyLoss \n",
    "                        'nll' for NLLLoss \\n''')\n",
    "\n",
    "print()\n",
    "optimizer_f = input('''Write the type of optimizer function you want.\n",
    "                        'sgd' for SGD \n",
    "                        'adam' for Adam\n",
    "                        'rms' for RMSprop\n",
    "                        'adagrad' for Adagrad \\n''')\n",
    "\n",
    "print(f'\\nSelected loss function: {loss_f} \\nSelected optimizer function: {optimizer_f}')\n",
    "criterion, optimizer = lossAndoptimizer(loss_f, optimizer_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of epochs. 20\n",
      "\n",
      "Training started.......\n",
      "\n",
      "ephoch 1, loss 2.2674\n",
      "-----------------------\n",
      "ephoch 2, loss 2.0279\n",
      "-----------------------\n",
      "ephoch 3, loss 1.4064\n",
      "-----------------------\n",
      "ephoch 4, loss 1.0002\n",
      "-----------------------\n",
      "ephoch 5, loss 0.3630\n",
      "-----------------------\n",
      "ephoch 6, loss 0.6165\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "def train_model(trainload, criteria, optimizer, epochs):\n",
    "    print('\\nTraining started.......\\n')\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0       # Set the running loss at each epoch to zero\n",
    "        for i, data in enumerate(trainload):\n",
    "            inputs, labels = data  # get the inputs; data is a list of [inputs, labels]\n",
    "            optimizer.zero_grad()  # clear the gradient\n",
    "            outputs = model(inputs) # feed the input and acquire the output from network\n",
    "            loss = criterion(outputs, labels) # calculating the predicted and the expected loss\n",
    "            loss.backward()    # Backpropagation\n",
    "            optimizer.step()    # update the parameters\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 0: # print every 2000 mini-batches\n",
    "                print(f\"ephoch {epoch + 1}, loss {loss.item():.4f}\")\n",
    "                print('-----------------------')\n",
    "    print('\\nFinished Training!')\n",
    "    return model\n",
    "    \n",
    "epochs = int(input('Enter the number of epochs. '))    \n",
    "model = train_model(train_load, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "get some random testing images\n",
    "make an iterator from test_loader\n",
    "Get a batch of training images\n",
    "\"\"\"\n",
    "images, labels = next(iter(test_load))\n",
    "\n",
    "# print images and labels\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(images)\n",
    "_, predicted = torch.max(results, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(batch_size)))\n",
    "\n",
    "fig2 = plt.figure()\n",
    "plt.subplots_adjust(top = 0.99)\n",
    "for i in range(batch_size):\n",
    "    fig2.add_subplot(2,2, i+1)\n",
    "    plt.title('truth ' + classes[labels[i]] + ': predict ' + classes[predicted[i]])\n",
    "    img = images[i] / 2 + 0.5     # this is to unnormalize the image\n",
    "    img = torchvision.transforms.ToPILImage()(img)\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network performance\n",
    "def accuracy(model, test_load):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_load:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total, correct, total\n",
    "\n",
    "acc, correct, total  = accuracy(model, test_load)\n",
    "print(f'Accuracy of the network on the {total} test images is: {acc}')\n",
    "print(f'{correct} images out of {total} are correctly predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison b/n different loss and optimizer\n",
    "| epoch |Loss  | Optimizer | Accuracy |\n",
    "|:------|:-----------:|:----------:|:------|\n",
    "|20|CrossEntropyLoss|SGD|35.23809523809524|\n",
    "|20|CrossEntropyLoss|Adam|39.04761904761905|\n",
    "|20|CrossEntropyLoss|RMSprop|37.142857142857146|\n",
    "|20|CrossEntropyLoss|Adagrad|38.095238095238095|\n",
    "|20|NLLLoss|SGD|42.38095238095238|\n",
    "|20|NLLLoss|Adam|27.142857142857142|\n",
    "|20|NLLLoss|RMSprop|24.285714285714285|\n",
    "|20|NLLLoss|Adagrad|29.523809523809526|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
